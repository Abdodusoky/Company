{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJc0Qc15UWKaDvmg/3Edwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdodusoky/Company/blob/main/Nasa_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# space_bio_knowledge_engine_app.py\n",
        "\"\"\"\n",
        "Space Biology Knowledge Engine — Streamlit app with CLI fallback\n",
        "\n",
        "This file fixes environments where `streamlit` (or other optional libs) are not installed.\n",
        "Behavior:\n",
        "- If `streamlit` is available, the full interactive Streamlit app will be used (same functionality as before).\n",
        "- If `streamlit` is NOT available, the script falls back to a command-line interface (CLI) that:\n",
        "  - Loads the dataset (default: /mnt/data/SB_publication_PMC.csv)\n",
        "  - Builds topics (TF-IDF + NMF)\n",
        "  - Builds embeddings (SentenceTransformers if available; otherwise TF-IDF vectors)\n",
        "  - Allows running a search query from the CLI and saves outputs to disk\n",
        "\n",
        "How to run (Streamlit UI, if streamlit installed):\n",
        "    streamlit run space_bio_knowledge_engine_app.py\n",
        "\n",
        "How to run (CLI fallback -- works without streamlit):\n",
        "    python space_bio_knowledge_engine_app.py --help\n",
        "\n",
        "This file intentionally avoids hard crashes when optional packages are missing and provides helpful console output.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import textwrap\n",
        "import re\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# Optional UI: streamlit\n",
        "try:\n",
        "    import streamlit as st\n",
        "    STREAMLIT_AVAILABLE = True\n",
        "except Exception:\n",
        "    st = None\n",
        "    STREAMLIT_AVAILABLE = False\n",
        "\n",
        "# Standard data science libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import plotly.express as px\n",
        "\n",
        "# Optional heavy libs\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    SSBERT_AVAILABLE = True\n",
        "except Exception:\n",
        "    SentenceTransformer = None\n",
        "    SSBERT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    NLTK_AVAILABLE = True\n",
        "except Exception:\n",
        "    NLTK_AVAILABLE = False\n",
        "\n",
        "# optional OpenAI & transformers\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "except Exception:\n",
        "    pipeline = None\n",
        "\n",
        "# Provide caching decorator fallbacks when streamlit is not available\n",
        "if STREAMLIT_AVAILABLE:\n",
        "    cache_resource = st.cache_resource\n",
        "    cache_data = st.cache_data\n",
        "else:\n",
        "    def cache_resource(f):\n",
        "        return f\n",
        "    def cache_data(f):\n",
        "        return f\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / Constants\n",
        "# ---------------------------\n",
        "DATA_PATH_DEFAULT = os.environ.get('SB_DATA_PATH', 'SB_publication_PMC.csv')\n",
        "EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'\n",
        "NUM_TOPICS = 8\n",
        "TOP_K = 5\n",
        "\n",
        "# ---------------------------\n",
        "# Utility classes & functions\n",
        "# ---------------------------\n",
        "class EmbeddingModelWrapper:\n",
        "    \"\"\"Wrapper that uses SentenceTransformers if available, otherwise TF-IDF vectors as a fallback.\n",
        "\n",
        "    The encode(...) method returns a numpy array suitable for cosine similarity.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, tfidf_max_features: int = 10000):\n",
        "        self.kind = 'sbert' if SSBERT_AVAILABLE else 'tfidf'\n",
        "        self.model_name = model_name\n",
        "        if self.kind == 'sbert':\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "            self.tfidf_vectorizer = None\n",
        "        else:\n",
        "            self.model = None\n",
        "            self.tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features, stop_words='english')\n",
        "\n",
        "    def fit_transform_for_fallback(self, texts: List[str]) -> np.ndarray:\n",
        "        # Fit TF-IDF on texts and return dense array\n",
        "        X = self.tfidf_vectorizer.fit_transform(texts)\n",
        "        return X.toarray()\n",
        "\n",
        "    def encode(self, texts: List[str], convert_to_numpy: bool = True, show_progress_bar: bool = False) -> np.ndarray:\n",
        "        if self.kind == 'sbert':\n",
        "            return self.model.encode(texts, convert_to_numpy=convert_to_numpy, show_progress_bar=show_progress_bar)\n",
        "        else:\n",
        "            # If our vectorizer hasn't been fit yet, fit it on provided texts (caller usually passes full corpus)\n",
        "            try:\n",
        "                X = self.tfidf_vectorizer.transform(texts)\n",
        "            except Exception:\n",
        "                X = self.tfidf_vectorizer.fit_transform(texts)\n",
        "            return X.toarray()\n",
        "\n",
        "\n",
        "def load_data(path: str = DATA_PATH_DEFAULT, nrows: Optional[int] = None) -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {path}. Place the CSV there or provide --data-path.\")\n",
        "    df = pd.read_csv(path, nrows=nrows)\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_row(row: pd.Series) -> str:\n",
        "    parts = []\n",
        "    for col in ['title', 'abstract', 'body', 'authors', 'journal', 'year']:\n",
        "        if col in row and pd.notnull(row[col]):\n",
        "            parts.append(str(row[col]))\n",
        "    text = '\\n'.join(parts)\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_corpus(df: pd.DataFrame, text_col: str = 'combined_text') -> List[str]:\n",
        "    if text_col not in df.columns:\n",
        "        df[text_col] = df.apply(preprocess_row, axis=1)\n",
        "    texts = df[text_col].fillna('').astype(str).tolist()\n",
        "    return texts\n",
        "\n",
        "# ---------------------------\n",
        "# Embeddings + Topics\n",
        "# ---------------------------\n",
        "@cache_resource\n",
        "def get_embedding_model() -> EmbeddingModelWrapper:\n",
        "    return EmbeddingModelWrapper()\n",
        "\n",
        "@cache_resource\n",
        "def build_embeddings(texts: List[str], model_wrapper: EmbeddingModelWrapper) -> np.ndarray:\n",
        "    if model_wrapper.kind == 'tfidf':\n",
        "        # Fit TF-IDF on the entire corpus to get consistent vectors\n",
        "        emb = model_wrapper.fit_transform_for_fallback(texts)\n",
        "    else:\n",
        "        emb = model_wrapper.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "    # normalize rows to unit vectors to use cosine similarity with dot product\n",
        "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    emb = emb / norms\n",
        "    return emb\n",
        "\n",
        "@cache_data\n",
        "def make_topics(texts: List[str], n_topics: int = NUM_TOPICS, n_features: int = 5000) -> Tuple[List[Tuple[int, List[str]]], np.ndarray]:\n",
        "    tfidf = TfidfVectorizer(max_features=n_features, stop_words='english')\n",
        "    X = tfidf.fit_transform(texts)\n",
        "    nmf = NMF(n_components=n_topics, random_state=0)\n",
        "    W = nmf.fit_transform(X)\n",
        "    H = nmf.components_\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "        topics.append((topic_idx, top_features))\n",
        "    return topics, W\n",
        "\n",
        "# ---------------------------\n",
        "# Summarization & generation helpers\n",
        "# ---------------------------\n",
        "def call_openai_completion(prompt: str, max_tokens: int = 256, temperature: float = 0.2) -> Optional[str]:\n",
        "    key = os.environ.get('OPENAI_API_KEY')\n",
        "    if not key or openai is None:\n",
        "        return None\n",
        "    openai.api_key = key\n",
        "    try:\n",
        "        # Use a generic ChatCompletion call if available\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model='gpt-4o' if getattr(openai, 'ChatCompletion', None) else 'gpt-3.5-turbo',\n",
        "            messages=[{'role':'user','content':prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def local_summarize(text: str, max_length: int = 150) -> Optional[str]:\n",
        "    if pipeline is None:\n",
        "        return None\n",
        "    try:\n",
        "        summarizer = pipeline('summarization', model='facebook/bart-large-cnn', truncation=True)\n",
        "        out = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "        return out[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Local summarization failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------\n",
        "# Search & extractive answer\n",
        "# ---------------------------\n",
        "\n",
        "def search_texts(query: str, texts: List[str], embeddings: np.ndarray, model_wrapper: EmbeddingModelWrapper, top_k: int = TOP_K) -> List[Tuple[int, float]]:\n",
        "    q_emb = model_wrapper.encode([query], convert_to_numpy=True)\n",
        "    # normalize\n",
        "    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)\n",
        "    sims = cosine_similarity(q_emb, embeddings)[0]\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    return [(int(i), float(sims[i])) for i in top_idx]\n",
        "\n",
        "\n",
        "def extractive_answer(user_question: str, texts: List[str], embeddings: np.ndarray, model_wrapper: EmbeddingModelWrapper, top_k: int = TOP_K) -> Tuple[List[str], np.ndarray]:\n",
        "    hits = search_texts(user_question, texts, embeddings, model_wrapper, top_k=top_k)\n",
        "    indices = [i for i, _ in hits]\n",
        "    scores = np.array([s for _, s in hits])\n",
        "    answers = [texts[i] for i in indices]\n",
        "    return answers, scores\n",
        "\n",
        "# ---------------------------\n",
        "# Streamlit UI (if available)\n",
        "# ---------------------------\n",
        "\n",
        "def main_streamlit(data_path: str = DATA_PATH_DEFAULT):\n",
        "    # This function assumes streamlit is available.\n",
        "    st.set_page_config(layout='wide', page_title='Space Biology Knowledge Engine')\n",
        "    st.title('🚀 Space Biology Knowledge Engine — Research explorer & chatbot')\n",
        "\n",
        "    st.sidebar.header('Configuration')\n",
        "    use_sample = st.sidebar.checkbox('Load sample subset (fast)', value=True)\n",
        "    nrows = 200 if use_sample else None\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        st.error(f\"Dataset not found at {data_path}. Please upload or place it there.\")\n",
        "        return\n",
        "\n",
        "    df = load_data(data_path, nrows=nrows)\n",
        "\n",
        "    if 'combined_text' not in df.columns:\n",
        "        st.info('Combining columns into `combined_text` (title, abstract, body, authors)')\n",
        "        df['combined_text'] = df.apply(preprocess_row, axis=1)\n",
        "\n",
        "    with st.expander('Dataset preview'):\n",
        "        st.dataframe(df.head(10))\n",
        "\n",
        "    if st.sidebar.button('Build topics'):\n",
        "        with st.spinner('Building topics (TF-IDF + NMF)...'):\n",
        "            texts = prepare_corpus(df)\n",
        "            topics, W = make_topics(texts)\n",
        "            st.subheader('Topics (TF-IDF + NMF)')\n",
        "            for tid, words in topics:\n",
        "                st.markdown(f\"**Topic {tid}**: {', '.join(words)}\")\n",
        "\n",
        "    if st.sidebar.button('Build embeddings'):\n",
        "        with st.spinner('Loading embedding model and encoding texts...'):\n",
        "            model_wrapper = get_embedding_model()\n",
        "            texts = prepare_corpus(df)\n",
        "            embeddings = build_embeddings(texts, model_wrapper)\n",
        "            st.session_state['embeddings'] = embeddings\n",
        "            st.session_state['model_wrapper'] = model_wrapper\n",
        "            st.success('Embeddings built and cached')\n",
        "\n",
        "    # Search\n",
        "    st.header('🔎 Search & Explore')\n",
        "    query = st.text_input('Enter a search question or keywords')\n",
        "    k = st.slider('Results to show', min_value=1, max_value=20, value=5)\n",
        "\n",
        "    if st.button('Search') and query.strip():\n",
        "        if 'embeddings' not in st.session_state:\n",
        "            st.warning('Please build embeddings first (sidebar).')\n",
        "        else:\n",
        "            model_wrapper = st.session_state['model_wrapper']\n",
        "            embeddings = st.session_state['embeddings']\n",
        "            texts = prepare_corpus(df)\n",
        "            hits = search_texts(query, texts, embeddings, model_wrapper, top_k=k)\n",
        "            st.write(f'Found top {len(hits)} documents:')\n",
        "            for rank, (idx, score) in enumerate(hits, start=1):\n",
        "                row = df.iloc[idx]\n",
        "                st.markdown(f\"**{rank}. {row.get('title','(no title)')}** — score {score:.3f}\")\n",
        "                if 'abstract' in row and pd.notnull(row['abstract']):\n",
        "                    st.write(row['abstract'][:800] + ('...' if len(str(row['abstract']))>800 else ''))\n",
        "\n",
        "    # Chatbot (RAG) - simplified\n",
        "    st.header('🤖 Research Chatbot (RAG)')\n",
        "    user_q = st.text_area('Ask a research question about space biology (use the dataset)')\n",
        "    bot_mode = st.radio('Answering mode', ['Retrieval + Extractive', 'Retrieval + Generative (OpenAI)', 'Local Generative (HF)'])\n",
        "\n",
        "    if st.button('Ask') and user_q.strip():\n",
        "        if 'embeddings' not in st.session_state:\n",
        "            st.warning('Please build embeddings first (sidebar).')\n",
        "        else:\n",
        "            model_wrapper = st.session_state['model_wrapper']\n",
        "            embeddings = st.session_state['embeddings']\n",
        "            texts = prepare_corpus(df)\n",
        "            answers, scores = extractive_answer(user_q, texts, embeddings, model_wrapper, top_k=TOP_K)\n",
        "            st.subheader('Retrieved (extractive)')\n",
        "            for i, (a, s) in enumerate(zip(answers, scores), start=1):\n",
        "                st.markdown(f\"**Snippet {i} (score {s:.3f})**\")\n",
        "                st.write(textwrap.shorten(a, width=600, placeholder='...'))\n",
        "\n",
        "            if bot_mode == 'Retrieval + Generative (OpenAI)':\n",
        "                prompt = 'You are an expert assistant for space biology research. Use the following retrieved documents as evidence, then answer the user question succinctly.\\n\\n'\n",
        "                for j, doc in enumerate(answers, start=1):\n",
        "                    prompt += f\"[DOC {j}] \" + doc[:1000] + '\\n\\n'\n",
        "                prompt += f\"User question: {user_q}\\n\\nAnswer concisely, cite which DOC number you used.\"\n",
        "                out = call_openai_completion(prompt)\n",
        "                if out:\n",
        "                    st.subheader('Generative answer (OpenAI)')\n",
        "                    st.write(out)\n",
        "                else:\n",
        "                    st.warning('OpenAI not available or call failed. Consider extractive mode.')\n",
        "\n",
        "            if bot_mode == 'Local Generative (HF)':\n",
        "                if pipeline is None:\n",
        "                    st.warning('Local transformers pipeline not available.')\n",
        "                else:\n",
        "                    gen_pipe = pipeline('text-generation', model='gpt2', device=-1)\n",
        "                    context = '\\n\\n'.join(answers[:3])\n",
        "                    prompt = f\"Context: {context}\\n\\nQuestion: {user_q}\\nAnswer:\"\n",
        "                    res = gen_pipe(prompt, max_length=256, do_sample=False)\n",
        "                    st.subheader('Local generative answer')\n",
        "                    st.write(res[0]['generated_text'])\n",
        "\n",
        "    # Visualization (Streamlit only)\n",
        "    st.header('📊 Interactive Research Visualizations')\n",
        "    viz_choice = st.selectbox('Choose visualization', ['Publications per year', 'Top authors', 'Topic word list'])\n",
        "\n",
        "    if viz_choice == 'Publications per year':\n",
        "        if 'year' in df.columns:\n",
        "            df_year = df.dropna(subset=['year'])\n",
        "            try:\n",
        "                df_year['year'] = df_year['year'].astype(int)\n",
        "                counts = df_year.groupby('year').size().reset_index(name='count')\n",
        "                fig = px.bar(counts, x='year', y='count', title='Publications per year')\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "            except Exception:\n",
        "                st.info('Year column found but could not be converted to int.')\n",
        "        else:\n",
        "            st.info('No `year` column found in dataset')\n",
        "\n",
        "    if viz_choice == 'Top authors':\n",
        "        if 'authors' in df.columns:\n",
        "            all_auth = df['authors'].dropna().astype(str).str.split('[;,]').explode().str.strip()\n",
        "            top = all_auth.value_counts().head(20).reset_index()\n",
        "            top.columns = ['author','count']\n",
        "            fig = px.bar(top, x='author', y='count', title='Top authors', labels={'author':'Author', 'count':'Papers'})\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        else:\n",
        "            st.info('No `authors` column found in dataset')\n",
        "\n",
        "    if viz_choice == 'Topic word list':\n",
        "        texts = prepare_corpus(df)\n",
        "        topics, W = make_topics(texts)\n",
        "        cols = st.columns(2)\n",
        "        for i, (tid, words) in enumerate(topics):\n",
        "            with cols[i % 2]:\n",
        "                st.markdown(f\"### Topic {tid}\")\n",
        "                st.write(', '.join(words))\n",
        "\n",
        "    # Export\n",
        "    st.header('📁 Export')\n",
        "    st.write('Enter comma-separated indices to export (e.g. 0,5,10)')\n",
        "    indices_text = st.text_input('Indices to export')\n",
        "    if st.button('Export selected'):\n",
        "        try:\n",
        "            idxs = [int(x.strip()) for x in indices_text.split(',') if x.strip() != '']\n",
        "            sub = df.iloc[idxs]\n",
        "            csv = sub.to_csv(index=False).encode('utf-8')\n",
        "            st.download_button('Download CSV', data=csv, file_name='selected_papers.csv', mime='text/csv')\n",
        "        except Exception as e:\n",
        "            st.error(f'Failed to export: {e}')\n",
        "\n",
        "    st.sidebar.markdown('---')\n",
        "    st.sidebar.write('Tips: Build embeddings once, then use search/chat. For high-quality generation, set OPENAI_API_KEY.')\n",
        "\n",
        "# ---------------------------\n",
        "# CLI fallback\n",
        "# ---------------------------\n",
        "\n",
        "def print_table(df: pd.DataFrame, n: int = 5):\n",
        "    print(df.head(n).to_string(index=False))\n",
        "\n",
        "\n",
        "def main_cli(args: argparse.Namespace):\n",
        "    data_path = args.data_path or DATA_PATH_DEFAULT\n",
        "    nrows = args.nrows\n",
        "    try:\n",
        "        df = load_data(data_path, nrows=nrows)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if 'combined_text' not in df.columns:\n",
        "        df['combined_text'] = df.apply(preprocess_row, axis=1)\n",
        "\n",
        "    texts = prepare_corpus(df)\n",
        "    print(f\"Loaded {len(df)} rows from {data_path}\")\n",
        "\n",
        "    # Build topics\n",
        "    print('\\nBuilding topics (TF-IDF + NMF)...')\n",
        "    topics, W = make_topics(texts, n_topics=args.num_topics)\n",
        "    print('\\nTop topics:')\n",
        "    for tid, words in topics:\n",
        "        print(f\"Topic {tid}: {', '.join(words)}\")\n",
        "\n",
        "    # Build embeddings\n",
        "    print('\\nBuilding embeddings (SentenceTransformers if available, otherwise TF-IDF fallback)...')\n",
        "    model_wrapper = get_embedding_model()\n",
        "    embeddings = build_embeddings(texts, model_wrapper)\n",
        "    print(f\"Embeddings shape: {embeddings.shape} (model kind={model_wrapper.kind})\")\n",
        "\n",
        "    # Save embeddings optionally\n",
        "    if args.save_embeddings:\n",
        "        np.save(args.save_embeddings, embeddings)\n",
        "        print(f\"Saved embeddings to {args.save_embeddings}\")\n",
        "\n",
        "    # Quick search if query provided\n",
        "    if args.query:\n",
        "        hits = search_texts(args.query, texts, embeddings, model_wrapper, top_k=args.top_k)\n",
        "        print(f\"\\nTop {len(hits)} results for query: '{args.query}'\")\n",
        "        for rank, (idx, score) in enumerate(hits, start=1):\n",
        "            row = df.iloc[idx]\n",
        "            title = row.get('title', '(no title)')\n",
        "            abstract = row.get('abstract', '')\n",
        "            print(f\"{rank}. index={idx} score={score:.4f} title={title}\")\n",
        "            if pd.notnull(abstract):\n",
        "                print(textwrap.fill(str(abstract)[:400], width=120))\n",
        "                print('---')\n",
        "\n",
        "    # Run a simple smoke test if requested\n",
        "    if args.smoke_test:\n",
        "        sample_q = args.query or 'microgravity'\n",
        "        print(f\"\\nRunning smoke test with query '{sample_q}'\")\n",
        "        hits = search_texts(sample_q, texts, embeddings, model_wrapper, top_k=3)\n",
        "        print('Smoke test results:')\n",
        "        for i, (idx, score) in enumerate(hits, start=1):\n",
        "            print(f\"{i}. idx={idx} score={score:.4f} title={df.iloc[idx].get('title','(no title)')}\")\n",
        "\n",
        "    print('\\nCLI run finished.')\n",
        "\n",
        "# ---------------------------\n",
        "# Argument parsing & entry point\n",
        "# ---------------------------\n",
        "\n",
        "def make_argparser() -> argparse.ArgumentParser:\n",
        "    p = argparse.ArgumentParser(description='Space Biology Knowledge Engine (Streamlit UI if available; CLI fallback otherwise)')\n",
        "    p.add_argument('--data-path', type=str, default=DATA_PATH_DEFAULT, help='Path to CSV dataset')\n",
        "    p.add_argument('--nrows', type=int, default=None, help='Read only first N rows (for quick tests)')\n",
        "    p.add_argument('--num-topics', type=int, default=NUM_TOPICS, help='Number of NMF topics')\n",
        "    p.add_argument('--query', type=str, default=None, help='Run a quick search query (CLI mode)')\n",
        "    p.add_argument('--top-k', type=int, default=TOP_K, help='Number of top results to show')\n",
        "    p.add_argument('--save-embeddings', type=str, default=None, help='Save embeddings to .npy file')\n",
        "    p.add_argument('--smoke-test', action='store_true', help='Run a small smoke test after building embeddings')\n",
        "    return p\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = make_argparser()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    if STREAMLIT_AVAILABLE:\n",
        "        # If streamlit available and script is executed with `streamlit run`, run the UI function.\n",
        "        # When using `python script.py` in an environment with streamlit, we will still launch CLI by default.\n",
        "        invoked_via_streamlit = 'STREAMLIT_SERVER_RUN' in os.environ or 'streamlit' in sys.argv[0]\n",
        "        if invoked_via_streamlit:\n",
        "            # Streamlit expects to execute top-level code; define a wrapper\n",
        "            main_streamlit(data_path=args.data_path)\n",
        "        else:\n",
        "            # User ran `python script.py` in environment with streamlit; provide CLI as default\n",
        "            main_cli(args)\n",
        "    else:\n",
        "        # No streamlit — run CLI fallback\n",
        "        main_cli(args)\n",
        "\n",
        "# ---------------------------\n",
        "# End of file\n",
        "# ---------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv5Lxyhfpgjt",
        "outputId": "90b13171-0fe0-45c1-8394-3a643fc92660"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2025-09-17 12:47:43.569 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
            "2025-09-17 12:47:43.573 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.576 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.826 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-09-17 12:47:43.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.833 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.838 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.844 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.846 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8f8jUtvqKPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}