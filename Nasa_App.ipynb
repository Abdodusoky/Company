{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqWDwXTWNdz9cu85Z6Anew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdodusoky/Company/blob/main/Nasa_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# space_bio_knowledge_engine_app.py\n",
        "\"\"\"\n",
        "Streamlit app: Build a Space Biology Knowledge Engine (FAISS replaced with sklearn cosine similarity)\n",
        "- Loads dataset: /mnt/data/SB_publication_PMC.csv (assumed to be PubMed Central-style CSV)\n",
        "- Preprocesses text (title, abstract, body)\n",
        "- Builds embeddings with sentence-transformers\n",
        "- Provides: search, chatbot (RAG), extractive summarizer, abstractive summarizer (OpenAI or local HF), interactive graphs (Plotly), topic modeling (TF-IDF + NMF)\n",
        "\n",
        "How to run:\n",
        "1. Create a virtualenv and install requirements:\n",
        "   python -m venv venv\n",
        "   source venv/bin/activate   # or venv\\\\Scripts\\\\activate on Windows\n",
        "   pip install -r requirements.txt\n",
        "\n",
        "2. Example requirements.txt (also included below in comments):\n",
        "   streamlit\n",
        "   pandas\n",
        "   numpy\n",
        "   scikit-learn\n",
        "   sentence-transformers\n",
        "   plotly\n",
        "   nltk\n",
        "   openai\n",
        "   transformers\n",
        "   tiktoken\n",
        "\n",
        "3. Run:\n",
        "   streamlit run space_bio_knowledge_engine_app.py\n",
        "\n",
        "Notes:\n",
        "- If you want to use OpenAI for generation/summarization/chat, set OPENAI_API_KEY in environment variables.\n",
        "- If OpenAI is not available, the app will attempt to use local HuggingFace transformer models (may require large downloads).\n",
        "\n",
        "This file is intentionally self-contained for a hackathon MVP. Replace models or tweak parameters for production.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import plotly.express as px\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "\n",
        "# Optional imports for generation\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "except Exception:\n",
        "    pipeline = None\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / Constants\n",
        "# ---------------------------\n",
        "DATA_PATH = '/mnt/data/SB_publication_PMC.csv'  # user-provided CSV path\n",
        "EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'  # sentence-transformers\n",
        "EMBEDDING_DIM = 768  # mpnet dim\n",
        "NUM_TOPICS = 8\n",
        "TOP_K = 5\n",
        "\n",
        "# ---------------------------\n",
        "# Utility functions\n",
        "# ---------------------------\n",
        "\n",
        "def load_data(path=DATA_PATH, nrows=None):\n",
        "    df = pd.read_csv(path, nrows=nrows)\n",
        "    st.write(f\"Loaded dataset: {path} -- {len(df)} rows\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_row(row):\n",
        "    # Combine useful fields; adjust field names to dataset columns\n",
        "    parts = []\n",
        "    for col in ['title', 'abstract', 'body', 'authors', 'journal', 'year']:\n",
        "        if col in row and pd.notnull(row[col]):\n",
        "            parts.append(str(row[col]))\n",
        "    text = '\\n'.join(parts)\n",
        "    # basic cleaning\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_corpus(df, text_col='combined_text'):\n",
        "    texts = df[text_col].fillna('').astype(str).tolist()\n",
        "    return texts\n",
        "\n",
        "# ---------------------------\n",
        "# Embeddings\n",
        "# ---------------------------\n",
        "@st.cache_resource\n",
        "def load_embedding_model(name=EMBEDDING_MODEL_NAME):\n",
        "    model = SentenceTransformer(name)\n",
        "    return model\n",
        "\n",
        "@st.cache_resource\n",
        "def build_embeddings(texts, model):\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "    return embeddings\n",
        "\n",
        "# ---------------------------\n",
        "# Topic modeling (TF-IDF + NMF)\n",
        "# ---------------------------\n",
        "@st.cache_data\n",
        "def make_topics(texts, n_topics=NUM_TOPICS, n_features=5000):\n",
        "    tfidf = TfidfVectorizer(max_features=n_features, stop_words='english')\n",
        "    X = tfidf.fit_transform(texts)\n",
        "    nmf = NMF(n_components=n_topics, random_state=0)\n",
        "    W = nmf.fit_transform(X)\n",
        "    H = nmf.components_\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "        topics.append((topic_idx, top_features))\n",
        "    return topics, W\n",
        "\n",
        "# ---------------------------\n",
        "# Summarization + Chat helpers\n",
        "# ---------------------------\n",
        "\n",
        "def extractive_answer(user_question, docs_texts, embeddings_model, doc_embeddings, top_k=TOP_K):\n",
        "    q_emb = embeddings_model.encode([user_question], convert_to_numpy=True)\n",
        "    sims = cosine_similarity(q_emb, doc_embeddings)[0]\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    answers = [docs_texts[i] for i in top_idx]\n",
        "    return answers, sims[top_idx]\n",
        "\n",
        "\n",
        "def call_openai_completion(prompt, max_tokens=256, temperature=0.2):\n",
        "    key = os.environ.get('OPENAI_API_KEY')\n",
        "    if not key or openai is None:\n",
        "        return None\n",
        "    openai.api_key = key\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model='gpt-4o-mini' if 'gpt-4o-mini' in openai.Model.list() else 'gpt-4o',\n",
        "            messages=[{'role':'user','content':prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        st.warning(f\"OpenAI call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def local_summarize(text, max_length=150):\n",
        "    if pipeline is None:\n",
        "        return None\n",
        "    try:\n",
        "        summarizer = pipeline('summarization', model='facebook/bart-large-cnn', truncation=True)\n",
        "        out = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "        return out[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Local summarization failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------\n",
        "# App UI\n",
        "# ---------------------------\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(layout='wide', page_title='Space Biology Knowledge Engine')\n",
        "    st.title('üöÄ Space Biology Knowledge Engine ‚Äî Research explorer & chatbot')\n",
        "\n",
        "    st.sidebar.header('Configuration')\n",
        "    use_sample = st.sidebar.checkbox('Load sample subset (fast)', value=True)\n",
        "    nrows = 200 if use_sample else None\n",
        "\n",
        "    if not os.path.exists(DATA_PATH):\n",
        "        st.error(f\"Dataset not found at {DATA_PATH}. Please upload or place it there.\")\n",
        "        return\n",
        "\n",
        "    df = load_data(DATA_PATH, nrows=nrows)\n",
        "\n",
        "    # Create combined_text column\n",
        "    if 'combined_text' not in df.columns:\n",
        "        st.info('Combining columns into `combined_text` (title, abstract, body, authors)')\n",
        "        df['combined_text'] = df.apply(preprocess_row, axis=1)\n",
        "\n",
        "    # show basic dataset preview\n",
        "    with st.expander('Dataset preview'):\n",
        "        st.dataframe(df.head(10))\n",
        "\n",
        "    # Topic modeling\n",
        "    if st.sidebar.button('Build topics'):\n",
        "        with st.spinner('Building topics (TF-IDF + NMF)...'):\n",
        "            texts = prepare_corpus(df)\n",
        "            topics, W = make_topics(texts, n_topics=NUM_TOPICS)\n",
        "            st.subheader('Topics (TF-IDF + NMF)')\n",
        "            for tid, words in topics:\n",
        "                st.markdown(f\"**Topic {tid}**: {', '.join(words)}\")\n",
        "\n",
        "    # Embeddings\n",
        "    st.sidebar.header('Embeddings')\n",
        "    if st.sidebar.button('Build embeddings'):\n",
        "        with st.spinner('Loading embedding model...'):\n",
        "            model = load_embedding_model()\n",
        "            texts = prepare_corpus(df)\n",
        "            st.info('Encoding texts (this may take a while)...')\n",
        "            embeddings = build_embeddings(texts, model)\n",
        "            st.session_state['embeddings'] = embeddings\n",
        "            st.session_state['sbert_model'] = model\n",
        "            st.success('Embeddings built and cached')\n",
        "\n",
        "    # Quick search interface\n",
        "    st.header('üîé Search & Explore')\n",
        "    query = st.text_input('Enter a search question or keywords')\n",
        "    k = st.slider('Results to show', min_value=1, max_value=20, value=5)\n",
        "\n",
        "    if st.button('Search') and query.strip():\n",
        "        if 'sbert_model' not in st.session_state:\n",
        "            st.warning('Please build embeddings first (sidebar).')\n",
        "        else:\n",
        "            model = st.session_state['sbert_model']\n",
        "            embeddings = st.session_state['embeddings']\n",
        "            q_emb = model.encode([query], convert_to_numpy=True)\n",
        "            sims = cosine_similarity(q_emb, embeddings)[0]\n",
        "            hits = sims.argsort()[::-1][:k]\n",
        "            st.write(f'Found top {len(hits)} documents:')\n",
        "            for rank, idx in enumerate(hits, start=1):\n",
        "                score = sims[idx]\n",
        "                row = df.iloc[idx]\n",
        "                st.markdown(f\"**{rank}. {row.get('title','(no title)')}** ‚Äî score {score:.3f}\")\n",
        "                if 'abstract' in row and pd.notnull(row['abstract']):\n",
        "                    st.write(row['abstract'][:800] + ('...' if len(str(row['abstract']))>800 else ''))\n",
        "                st.write(f\"[View full row] -> index {idx}\")\n",
        "\n",
        "    # Chatbot\n",
        "    st.header('ü§ñ Research Chatbot (RAG)')\n",
        "    user_q = st.text_area('Ask a research question about space biology (use the dataset)')\n",
        "    bot_mode = st.radio('Answering mode', ['Retrieval + Extractive', 'Retrieval + Generative (OpenAI)', 'Local Generative (HF)'])\n",
        "\n",
        "    if st.button('Ask') and user_q.strip():\n",
        "        if 'sbert_model' not in st.session_state:\n",
        "            st.warning('Please build embeddings first (sidebar).')\n",
        "        else:\n",
        "            model = st.session_state['sbert_model']\n",
        "            embeddings = st.session_state['embeddings']\n",
        "            texts = prepare_corpus(df)\n",
        "            # retrieve top docs\n",
        "            q_emb = model.encode([user_q], convert_to_numpy=True)\n",
        "            sims = cosine_similarity(q_emb, embeddings)[0]\n",
        "            hits = sims.argsort()[::-1][:TOP_K]\n",
        "            retrieved_texts = [texts[i] for i in hits]\n",
        "            st.subheader('Retrieved documents (short)')\n",
        "            for i, t in enumerate(retrieved_texts, start=1):\n",
        "                st.write(f\"{i}. \" + textwrap.shorten(t, width=400, placeholder='...'))\n",
        "\n",
        "            if bot_mode == 'Retrieval + Extractive':\n",
        "                answers, sims_top = extractive_answer(user_q, texts, model, embeddings, top_k=TOP_K)\n",
        "                st.subheader('Extractive answers (top snippets)')\n",
        "                for i, (a, s) in enumerate(zip(answers, sims_top), start=1):\n",
        "                    st.markdown(f\"**Snippet {i} (score {s:.3f})**\")\n",
        "                    st.write(textwrap.shorten(a, width=600, placeholder='...'))\n",
        "\n",
        "            elif bot_mode == 'Retrieval + Generative (OpenAI)':\n",
        "                prompt = 'You are an expert assistant for space biology research. Use the following retrieved documents as evidence, then answer the user question succinctly.\\n\\n'\n",
        "                for j, doc in enumerate(retrieved_texts, start=1):\n",
        "                    prompt += f\"[DOC {j}] \" + doc[:1000] + '\\n\\n'\n",
        "                prompt += f\"User question: {user_q}\\n\\nAnswer concisely, cite which DOC number you used.\"\n",
        "                out = call_openai_completion(prompt)\n",
        "                if out:\n",
        "                    st.subheader('Generative answer (OpenAI)')\n",
        "                    st.write(out)\n",
        "                else:\n",
        "                    st.warning('OpenAI not available or call failed. Consider local mode or extractive mode.')\n",
        "\n",
        "            elif bot_mode == 'Local Generative (HF)':\n",
        "                if pipeline is None:\n",
        "                    st.warning('transformers not installed/available. Install `transformers` and required models.')\n",
        "                else:\n",
        "                    gen_pipe = pipeline('text-generation', model='gpt2', device=-1)\n",
        "                    context = '\\n\\n'.join(retrieved_texts[:3])\n",
        "                    prompt = f\"Context: {context}\\n\\nQuestion: {user_q}\\nAnswer:\"\n",
        "                    res = gen_pipe(prompt, max_length=256, do_sample=False)\n",
        "                    st.subheader('Local generative answer')\n",
        "                    st.write(res[0]['generated_text'])\n",
        "\n",
        "    # Visualization\n",
        "    st.header('üìä Interactive Research Visualizations')\n",
        "    viz_choice = st.selectbox('Choose visualization', ['Publications per year', 'Top authors', 'Topic wordcloud-like list'])\n",
        "\n",
        "    if viz_choice == 'Publications per year':\n",
        "        if 'year' in df.columns:\n",
        "            df_year = df.dropna(subset=['year'])\n",
        "            df_year['year'] = df_year['year'].astype(int)\n",
        "            counts = df_year.groupby('year').size().reset_index(name='count')\n",
        "            fig = px.bar(counts, x='year', y='count', title='Publications per year')\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        else:\n",
        "            st.info('No `year` column found in dataset')\n",
        "\n",
        "    if viz_choice == 'Top authors':\n",
        "        if 'authors' in df.columns:\n",
        "            # naive split authors by semicolon or comma\n",
        "            all_auth = df['authors'].dropna().astype(str).str.split('[;,]').explode().str.strip()\n",
        "            top = all_auth.value_counts().head(20).reset_index()\n",
        "            top.columns = ['author','count']\n",
        "            fig = px.bar(top, x='author', y='count', title='Top authors', labels={'author':'Author', 'count':'Papers'})\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        else:\n",
        "            st.info('No `authors` column found in dataset')\n",
        "\n",
        "    if viz_choice == 'Topic wordcloud-like list':\n",
        "        texts = prepare_corpus(df)\n",
        "        topics, W = make_topics(texts, n_topics=NUM_TOPICS)\n",
        "        cols = st.columns(2)\n",
        "        for i, (tid, words) in enumerate(topics):\n",
        "            with cols[i % 2]:\n",
        "                st.markdown(f\"### Topic {tid}\")\n",
        "                st.write(', '.join(words))\n",
        "\n",
        "    # Export selected docs\n",
        "    st.header('üìÅ Export')\n",
        "    st.write('You can select rows to export as CSV (by index list)')\n",
        "    indices_text = st.text_input('Enter comma-separated indices to export (e.g. 0,5,10)')\n",
        "    if st.button('Export selected'):\n",
        "        try:\n",
        "            idxs = [int(x.strip()) for x in indices_text.split(',') if x.strip()!='']\n",
        "            sub = df.iloc[idxs]\n",
        "            csv = sub.to_csv(index=False).encode('utf-8')\n",
        "            st.download_button('Download CSV', data=csv, file_name='selected_papers.csv', mime='text/csv')\n",
        "        except Exception as e:\n",
        "            st.error(f'Failed to export: {e}')\n",
        "\n",
        "    st.sidebar.markdown('---')\n",
        "    st.sidebar.write('Tips: Build embeddings once, then use search/chat. For high-quality generation, set OPENAI_API_KEY.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "# ---------------------------\n",
        "# End of file\n",
        "# ---------------------------\n"
      ],
      "metadata": {
        "id": "Hv5Lxyhfpgjt",
        "outputId": "90b13171-0fe0-45c1-8394-3a643fc92660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2025-09-17 12:47:43.569 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
            "2025-09-17 12:47:43.573 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.576 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.826 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-09-17 12:47:43.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.833 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.838 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.844 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-17 12:47:43.846 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8f8jUtvqKPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}